{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"0208_low_level_tensorflow_api.ipynb","provenance":[{"file_id":"https://github.com/10sorflow/tf2_course/blob/master/02_low_level_tensorflow_api.ipynb","timestamp":1581313989634}],"collapsed_sections":["GqsSRDj1p6OW","Gjie1yNAp6Ow","LeFLw1B6p6PN","vHHuPOUup6Ph","2eOiQ3M7p6P-","O3Ir8bdvp6Qq","J4jNqNeDp6RL","k5Jh5iUXp6Rv","9ISQFxaVp6Sp","H1CiWyRwp6TU","4Jjjm6Ypp6Us","iCC9tLrrp6VN","r66J772ap6VV","iCjDWmLYp6VW","ddfpVMtYp6Vh","4QxObmtxp6V5","vOGYPHcjp6WH","lvgOWevVp6WR","iPtV_FiRp6WR","LhzV53gpp6Wb","mAe2Nuflp6We","lmLLoSW0p6Wk","VFz_SAbjp6Ww","V5k2qMB-p6Wx","WDhxWOsVp6Xm","WCuYVXX-p6Xu","wbkvi9NYp6YD","J8bp0hHJp6YM","YJHsMSiop6YS","M9o2F_zIp6YT","D2aNFiJMp6YU","tRP234A6p6Ym","7q_HQHtSp6Yn","auDCmOhcp6ZB","S7XeXGMep6ZU","im0euHQup6ZV","FmvqxxsJp6Ze","c5dt4jHfp6Zm","n_B_DEkip6Zq","W4-pj8V2p6Zx","JPrJEbcip6Z4","oD-q90LRp6Z9","QcPIfVWFp6Z-","rQwSZ4nMp6aI","dbq_V5v-p6aL","n4FICqpEp6aM","0JS6QHybp6aO","Kpz84Pqvp6aO","4geASMTXp6ap","iMAc2cIup6av","mPpJ1kWhp6a0","7V5ne-3Lp6a_","reDOANH4p6bM","ZnDb2ceqp6bM","3X3VSVwfp6bN","OWuOZwn_p6bQ","JgFeWQY5p6bR","NMaIocFSp6bU","nqmhe9Dyp6bd"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"lWmdb8itp6ME","colab_type":"text"},"source":["# Low-Level TensorFlow API"]},{"cell_type":"markdown","metadata":{"id":"QX_Vyduyp6MR","colab_type":"text"},"source":["In this notebook you will learn how to use TensorFlow's low-level API, then use it to build custom loss functions, as well as custom Keras layers and models."]},{"cell_type":"markdown","metadata":{"id":"e5aCuwOQp6MU","colab_type":"text"},"source":["## Imports"]},{"cell_type":"code","metadata":{"id":"W09iuH7kp6MW","colab_type":"code","colab":{}},"source":["%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h_vxC6ZmqVfC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"d1541398-b625-4844-def3-336287d8b9f0","executionInfo":{"status":"ok","timestamp":1581314393167,"user_tz":-540,"elapsed":577,"user":{"displayName":"박정은","photoUrl":"","userId":"10420422305486695936"}}},"source":["%tensorflow_version 2.x"],"execution_count":2,"outputs":[{"output_type":"stream","text":["TensorFlow 2.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MB5p0Fm9p6Mw","colab_type":"code","colab":{}},"source":["import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import pandas as pd\n","import sklearn\n","import sys\n","import tensorflow as tf\n","from tensorflow import keras\n","import time"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hvMPdXPcp6M8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":161},"outputId":"92c71e74-5aed-4aad-b936-4af14aacb167","executionInfo":{"status":"ok","timestamp":1581314405800,"user_tz":-540,"elapsed":4932,"user":{"displayName":"박정은","photoUrl":"","userId":"10420422305486695936"}}},"source":["print(\"python\", sys.version)\n","for module in mpl, np, pd, sklearn, tf, keras:\n","    print(module.__name__, module.__version__)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["python 3.6.9 (default, Nov  7 2019, 10:44:02) \n","[GCC 8.3.0]\n","matplotlib 3.1.3\n","numpy 1.17.5\n","pandas 0.25.3\n","sklearn 0.22.1\n","tensorflow 2.1.0\n","tensorflow_core.python.keras.api._v2.keras 2.2.4-tf\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WRsUdmKFp6NG","colab_type":"code","colab":{}},"source":["assert sys.version_info >= (3, 5) # Python ≥3.5 required\n","assert tf.__version__ >= \"2.0\"    # TensorFlow ≥2.0 required"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XCaHREw9p6NR","colab_type":"text"},"source":["## Tensors and operations"]},{"cell_type":"markdown","metadata":{"id":"Gl4wABwUp6NU","colab_type":"text"},"source":["You can browse through the code examples or jump directly to the exercises."]},{"cell_type":"markdown","metadata":{"id":"OTRFkiysp6NW","colab_type":"text"},"source":["### Tensors"]},{"cell_type":"code","metadata":{"id":"bC0tvXuop6Na","colab_type":"code","colab":{}},"source":["t = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n","t"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vLzTGJjFp6Nl","colab_type":"code","colab":{}},"source":["t.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ttEO00MEp6N0","colab_type":"code","colab":{}},"source":["t.dtype"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SfhCCP72p6N5","colab_type":"text"},"source":["### Indexing"]},{"cell_type":"code","metadata":{"id":"YXSya7-cp6N8","colab_type":"code","colab":{}},"source":["t[:, 1:]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ItydgByep6OK","colab_type":"code","colab":{}},"source":["t[..., 1, tf.newaxis]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GqsSRDj1p6OW","colab_type":"text"},"source":["### Ops"]},{"cell_type":"code","metadata":{"id":"crcHk4hHp6OY","colab_type":"code","colab":{}},"source":["t + 10"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"urZuE-Opp6Of","colab_type":"code","colab":{}},"source":["tf.square(t)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y33P92i0p6Or","colab_type":"code","colab":{}},"source":["t @ tf.transpose(t)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gjie1yNAp6Ow","colab_type":"text"},"source":["### To/From NumPy"]},{"cell_type":"code","metadata":{"id":"Z9yOkKsAp6O4","colab_type":"code","colab":{}},"source":["t.numpy()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y3pQY6Uzp6O-","colab_type":"code","colab":{}},"source":["a = np.array([[1., 2., 3.], [4., 5., 6.]])\n","tf.constant(a)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4KBlKhDAp6PJ","colab_type":"code","colab":{}},"source":["t = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n","np.square(t)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LeFLw1B6p6PN","colab_type":"text"},"source":["### Scalars"]},{"cell_type":"code","metadata":{"id":"p7og4RBmp6PO","colab_type":"code","colab":{}},"source":["t = tf.constant(2.718)\n","t"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Y0CVtC3p6PT","colab_type":"code","colab":{}},"source":["t.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1AY-6HEIp6Pc","colab_type":"code","colab":{}},"source":["t.numpy()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vHHuPOUup6Ph","colab_type":"text"},"source":["### Conflicting Types"]},{"cell_type":"code","metadata":{"id":"_VOcqhZwp6Pk","colab_type":"code","colab":{}},"source":["try:\n","    tf.constant(1) + tf.constant(1.0)\n","except tf.errors.InvalidArgumentError as ex:\n","    print(ex)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PTpKHB_1p6Pu","colab_type":"code","colab":{}},"source":["try:\n","    tf.constant(1.0, dtype=tf.float64) + tf.constant(1.0)\n","except tf.errors.InvalidArgumentError as ex:\n","    print(ex)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F4ykFOEdp6P1","colab_type":"code","colab":{}},"source":["t = tf.constant(1.0, dtype=tf.float64)\n","tf.cast(t, tf.float32) + tf.constant(1.0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2eOiQ3M7p6P-","colab_type":"text"},"source":["### Strings"]},{"cell_type":"code","metadata":{"id":"pve1KLdRp6QC","colab_type":"code","colab":{}},"source":["t = tf.constant(\"café\")\n","t"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PaA5Ao8Fp6QR","colab_type":"code","colab":{}},"source":["tf.strings.length(t)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"euiSinH1p6Qa","colab_type":"code","colab":{}},"source":["tf.strings.length(t, unit=\"UTF8_CHAR\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VoMIrJB0p6Qh","colab_type":"code","colab":{}},"source":["tf.strings.unicode_decode(t, \"UTF8\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O3Ir8bdvp6Qq","colab_type":"text"},"source":["### String arrays"]},{"cell_type":"code","metadata":{"id":"OwOTxTkjp6Qx","colab_type":"code","colab":{}},"source":["t = tf.constant([\"Café\", \"Coffee\", \"caffè\", \"咖啡\"])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rGvTSgVAp6Q-","colab_type":"code","colab":{}},"source":["tf.strings.length(t, unit=\"UTF8_CHAR\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oo6uwhZFp6RE","colab_type":"code","colab":{}},"source":["r = tf.strings.unicode_decode(t, \"UTF8\")\n","r"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J4jNqNeDp6RL","colab_type":"text"},"source":["### Ragged tensors"]},{"cell_type":"code","metadata":{"id":"xvrnJqEJp6RM","colab_type":"code","colab":{}},"source":["r = tf.ragged.constant([[11, 12], [21, 22, 23], [], [41]])\n","r"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LEREJ4rBp6RR","colab_type":"code","colab":{}},"source":["print(r)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2LlIV-iCp6RV","colab_type":"code","colab":{}},"source":["print(r[1])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_NSqBZEap6Rd","colab_type":"code","colab":{}},"source":["print(r[1:2])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DN4RtBuap6Rh","colab_type":"code","colab":{}},"source":["r2 = tf.ragged.constant([[51, 52], [], [71]])\n","print(tf.concat([r, r2], axis=0))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qDPlV6v5p6Rm","colab_type":"code","colab":{}},"source":["r3 = tf.ragged.constant([[13, 14, 15], [24], [], [42, 43]])\n","print(tf.concat([r, r3], axis=1))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CrI4OPuIp6Rs","colab_type":"code","colab":{}},"source":["r.to_tensor()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k5Jh5iUXp6Rv","colab_type":"text"},"source":["### Sparse tensors"]},{"cell_type":"code","metadata":{"id":"QXsGj48Hp6Rw","colab_type":"code","colab":{}},"source":["s = tf.SparseTensor(indices=[[0, 1], [1, 0], [2, 3]],\n","                    values=[1., 2., 3.],\n","                    dense_shape=[3, 4])\n","print(s)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tpZT-nxwp6R4","colab_type":"code","colab":{}},"source":["tf.sparse.to_dense(s)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lVcVoSLop6SE","colab_type":"code","colab":{}},"source":["s2 = s * 2.0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aqmlWG3op6SH","colab_type":"code","colab":{}},"source":["try:\n","    s3 = s + 1.\n","except TypeError as ex:\n","    print(ex)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ObokVhU4p6SN","colab_type":"code","colab":{}},"source":["s4 = tf.constant([[10., 20.], [30., 40.], [50., 60.], [70., 80.]])\n","tf.sparse.sparse_dense_matmul(s, s4)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zR8IoZTTp6SR","colab_type":"code","colab":{}},"source":["s5 = tf.SparseTensor(indices=[[0, 2], [0, 1]],\n","                     values=[1., 2.],\n","                     dense_shape=[3, 4])\n","print(s5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hht9HWrip6SY","colab_type":"code","colab":{}},"source":["try:\n","    tf.sparse.to_dense(s5)\n","except tf.errors.InvalidArgumentError as ex:\n","    print(ex)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"brBVnr_rp6Sk","colab_type":"code","colab":{}},"source":["s6 = tf.sparse.reorder(s5)\n","tf.sparse.to_dense(s6)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9ISQFxaVp6Sp","colab_type":"text"},"source":["### Variables"]},{"cell_type":"code","metadata":{"id":"rxbs3oWEp6Sr","colab_type":"code","colab":{}},"source":["v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])\n","v"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pEErAz3Zp6Sx","colab_type":"code","colab":{}},"source":["v.value()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VRvAA03Pp6S1","colab_type":"code","colab":{}},"source":["v.numpy()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uNKyd-G7p6S7","colab_type":"code","colab":{}},"source":["v.assign(2 * v)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GIH1q3wPp6TC","colab_type":"code","colab":{}},"source":["v[0, 1].assign(42)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gJY2JJABp6TH","colab_type":"code","colab":{}},"source":["v[1].assign([7., 8., 9.])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ht6FLlbep6TL","colab_type":"code","colab":{}},"source":["try:\n","    v[1] = [7., 8., 9.]\n","except TypeError as ex:\n","    print(ex)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JM9T5R_lp6TP","colab_type":"code","colab":{}},"source":["sparse_delta = tf.IndexedSlices(values=[[1., 2., 3.], [4., 5., 6.]],\n","                                indices=[1, 0])\n","v.scatter_update(sparse_delta)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kwrq_IYfp6TR","colab_type":"code","colab":{}},"source":["v.scatter_nd_update(indices=[[0, 0], [1, 2]],\n","                    updates=[100., 200.])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H1CiWyRwp6TU","colab_type":"text"},"source":["### Devices"]},{"cell_type":"code","metadata":{"id":"U-AePpl3p6TV","colab_type":"code","colab":{}},"source":["with tf.device(\"/cpu:0\"):\n","    t = tf.constant([[1., 2., 3.], [4., 5., 6.]])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CJBF5Bi1p6TY","colab_type":"code","colab":{}},"source":["t.device"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xQWeXIt6p6Ta","colab_type":"code","colab":{}},"source":["if tf.test.is_gpu_available():\n","    with tf.device(\"/gpu:0\"):\n","        t2 = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n","    print(t2.device)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9XFaMO1Dp6Th","colab_type":"text"},"source":["![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"]},{"cell_type":"markdown","metadata":{"id":"oW6PGkLNp6Tj","colab_type":"text"},"source":["## Exercise 1 – Custom loss function"]},{"cell_type":"markdown","metadata":{"id":"963KPj3Fp6Tk","colab_type":"text"},"source":["Let's start by loading and preparing the California housing dataset. We first load it, then split it into a training set, a validation set and a test set, and finally we scale it:"]},{"cell_type":"code","metadata":{"id":"mztvUvUTp6Tm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"b39c3441-f63e-4230-9e02-b705af73de64","executionInfo":{"status":"ok","timestamp":1581314444052,"user_tz":-540,"elapsed":7484,"user":{"displayName":"박정은","photoUrl":"","userId":"10420422305486695936"}}},"source":["from sklearn.datasets import fetch_california_housing\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","housing = fetch_california_housing()\n","X_train_full, X_test, y_train_full, y_test = train_test_split(\n","    housing.data, housing.target.reshape(-1, 1), random_state=42)\n","X_train, X_valid, y_train, y_valid = train_test_split(\n","    X_train_full, y_train_full, random_state=42)\n","\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_valid_scaled = scaler.transform(X_valid)\n","X_test_scaled = scaler.transform(X_test)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"-XvsluFop6Tp","colab_type":"text"},"source":["### 1.1)\n","Create an `my_mse()` function with two arguments: the true labels `y_true` and the model predictions `y_pred`. Make it return the mean squared error using TensorFlow operations. Note that you could write your own custom metrics in exactly the same way. **Tip**: recall that the MSE is the mean of the squares of prediction errors, which are the differences between the predictions and the labels, so you will need to use `tf.reduce_mean()` and `tf.square()`."]},{"cell_type":"code","metadata":{"id":"0gYILbRpp6Tp","colab_type":"code","colab":{}},"source":["def my_mse(y_true,y_pred):\n","  return tf.reduce_mean(tf.square(y_pred-y_true))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WgcZRsbEp6Ty","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jT1ng3Wtp6T1","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0hXJwW70p6T4","colab_type":"text"},"source":["### 1.2)\n","Compile the following model, passing it your custom loss function, then train it and evaluate it. **Tip**: don't forget to use the scaled sets."]},{"cell_type":"code","metadata":{"id":"1oD5uAz7p6T4","colab_type":"code","colab":{}},"source":["model = keras.models.Sequential([\n","    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n","    keras.layers.Dense(1),\n","])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CDuEb8wKp6T_","colab_type":"code","colab":{}},"source":["model.compile(loss=my_mse, optimizer=keras.optimizers.SGD(lr=1e-3))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IjE-LRd1p6UH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":413},"outputId":"9850cdac-10e9-4189-f2c3-4a9da3da072e","executionInfo":{"status":"ok","timestamp":1581314716951,"user_tz":-540,"elapsed":7277,"user":{"displayName":"박정은","photoUrl":"","userId":"10420422305486695936"}}},"source":["model.fit(X_train_scaled,y_train,epochs=10,validation_data=(X_valid_scaled,y_valid))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Train on 11610 samples, validate on 3870 samples\n","Epoch 1/10\n","11610/11610 [==============================] - 1s 97us/sample - loss: 1.9173 - val_loss: 0.7243\n","Epoch 2/10\n","11610/11610 [==============================] - 1s 53us/sample - loss: 0.6748 - val_loss: 0.6345\n","Epoch 3/10\n","11610/11610 [==============================] - 1s 54us/sample - loss: 0.6216 - val_loss: 0.6426\n","Epoch 4/10\n","11610/11610 [==============================] - 1s 53us/sample - loss: 0.5921 - val_loss: 0.5830\n","Epoch 5/10\n","11610/11610 [==============================] - 1s 57us/sample - loss: 0.5673 - val_loss: 0.5498\n","Epoch 6/10\n","11610/11610 [==============================] - 1s 51us/sample - loss: 0.5461 - val_loss: 0.5657\n","Epoch 7/10\n","11610/11610 [==============================] - 1s 54us/sample - loss: 0.5287 - val_loss: 0.5052\n","Epoch 8/10\n","11610/11610 [==============================] - 1s 53us/sample - loss: 0.5135 - val_loss: 0.4926\n","Epoch 9/10\n","11610/11610 [==============================] - 1s 53us/sample - loss: 0.5000 - val_loss: 0.4845\n","Epoch 10/10\n","11610/11610 [==============================] - 1s 52us/sample - loss: 0.4886 - val_loss: 0.4618\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f6ded630da0>"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"VGGCeFthp6UJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"outputId":"dac1797a-a3e1-4853-f7ef-72a1ee84774b","executionInfo":{"status":"ok","timestamp":1581314731155,"user_tz":-540,"elapsed":589,"user":{"displayName":"박정은","photoUrl":"","userId":"10420422305486695936"}}},"source":["model.evaluate(X_test_scaled,y_test)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["5160/5160 [==============================] - 0s 32us/sample - loss: 0.4741\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0.47413472399231077"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"XZSFAwkLp6UN","colab_type":"text"},"source":["### 1.3)\n","Try building and compiling the model again, this time adding `\"mse\"` (or equivalently `\"mean_squared_error\"` or `keras.losses.mean_squared_error`) to the list of additional metrics, then train the model and make sure the `my_mse` is equal to the standard `mse`."]},{"cell_type":"code","metadata":{"id":"OfcuXKcHp6UN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":451},"outputId":"1e3974c0-363f-4d66-c610-0c7fae325757","executionInfo":{"status":"ok","timestamp":1581314820630,"user_tz":-540,"elapsed":7422,"user":{"displayName":"박정은","photoUrl":"","userId":"10420422305486695936"}}},"source":["model = keras.models.Sequential([\n","    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n","    keras.layers.Dense(1),\n","])\n","model.compile(loss=my_mse, optimizer=keras.optimizers.SGD(lr=1e-3),\n","              metrics=[\"mean_squared_error\"])\n","model.fit(X_train_scaled, y_train, epochs=10,\n","          validation_data=(X_valid_scaled, y_valid))\n","model.evaluate(X_test_scaled, y_test)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Train on 11610 samples, validate on 3870 samples\n","Epoch 1/10\n","11610/11610 [==============================] - 1s 75us/sample - loss: 2.2366 - mean_squared_error: 2.2366 - val_loss: 1.4665 - val_mean_squared_error: 1.4665\n","Epoch 2/10\n","11610/11610 [==============================] - 1s 55us/sample - loss: 0.9067 - mean_squared_error: 0.9067 - val_loss: 1.1262 - val_mean_squared_error: 1.1262\n","Epoch 3/10\n","11610/11610 [==============================] - 1s 55us/sample - loss: 0.7441 - mean_squared_error: 0.7441 - val_loss: 0.8230 - val_mean_squared_error: 0.8230\n","Epoch 4/10\n","11610/11610 [==============================] - 1s 55us/sample - loss: 0.6786 - mean_squared_error: 0.6786 - val_loss: 0.6845 - val_mean_squared_error: 0.6845\n","Epoch 5/10\n","11610/11610 [==============================] - 1s 54us/sample - loss: 0.6354 - mean_squared_error: 0.6354 - val_loss: 0.6553 - val_mean_squared_error: 0.6553\n","Epoch 6/10\n","11610/11610 [==============================] - 1s 53us/sample - loss: 0.6001 - mean_squared_error: 0.6001 - val_loss: 0.5989 - val_mean_squared_error: 0.5989\n","Epoch 7/10\n","11610/11610 [==============================] - 1s 54us/sample - loss: 0.5700 - mean_squared_error: 0.5700 - val_loss: 0.5519 - val_mean_squared_error: 0.5519\n","Epoch 8/10\n","11610/11610 [==============================] - 1s 54us/sample - loss: 0.5450 - mean_squared_error: 0.5450 - val_loss: 0.5628 - val_mean_squared_error: 0.5628\n","Epoch 9/10\n","11610/11610 [==============================] - 1s 55us/sample - loss: 0.5234 - mean_squared_error: 0.5234 - val_loss: 0.5821 - val_mean_squared_error: 0.5821\n","Epoch 10/10\n","11610/11610 [==============================] - 1s 54us/sample - loss: 0.5049 - mean_squared_error: 0.5049 - val_loss: 0.4908 - val_mean_squared_error: 0.4908\n","5160/5160 [==============================] - 0s 34us/sample - loss: 0.5059 - mean_squared_error: 0.5059\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.5058538595835368, 0.5058539]"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"0jvLFvV-p6US","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yHN4tCE-p6UX","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vvdMWMZap6Ua","colab_type":"text"},"source":["### 1.4)\n","If you want your code to be portable to other Python implementations of the Keras API, you should use the operations in `keras.backend` rather than TensorFlow operations directly. This package contains thin wrappers around the backend's operations (for example, `keras.backend.square()` simply calls `tf.square()`). Try reimplementing the `my_mse()` function this way and use it to train and evaluate your model again. **Tip**: people frequently define `K = keras.backend` to make their code more readable."]},{"cell_type":"code","metadata":{"id":"vKlesihbp6Ub","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pYCyYNU_p6Ug","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hQukZ4_Ap6Un","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RFEQ3z6gp6Ur","colab_type":"text"},"source":["![Exercise solution](https://camo.githubusercontent.com/250388fde3fac9135ead9471733ee28e049f7a37/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f302f30362f46696c6f735f736567756e646f5f6c6f676f5f253238666c69707065642532392e6a7067)"]},{"cell_type":"markdown","metadata":{"id":"4Jjjm6Ypp6Us","colab_type":"text"},"source":["### 1.1)\n","Create an `my_mse()` function with two arguments: the true labels `y_true` and the model predictions `y_pred`. Make it return the mean squared error using TensorFlow operations. Note that you could write your own custom metrics in exactly the same way. **Tip**: recall that the MSE is the mean of the squares of prediction errors, which are the differences between the predictions and the labels, so you will need to use `tf.reduce_mean()` and `tf.square()`."]},{"cell_type":"code","metadata":{"id":"IA40Nrypp6Ut","colab_type":"code","colab":{}},"source":["def my_mse(y_true, y_pred):\n","    return tf.reduce_mean(tf.square(y_pred - y_true))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q_9328FVp6Uv","colab_type":"text"},"source":["### 1.2)\n","Compile your model, passing it your custom loss function, then train it and evaluate it. **Tip**: don't forget to use the scaled sets."]},{"cell_type":"code","metadata":{"id":"sX8cT9NNp6Ux","colab_type":"code","colab":{}},"source":["model = keras.models.Sequential([\n","    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n","    keras.layers.Dense(1),\n","])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"a36C-dtrp6U1","colab_type":"code","colab":{}},"source":["model.compile(loss=my_mse, optimizer=keras.optimizers.SGD(lr=1e-3))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"z9vO6s-Bp6U-","colab_type":"code","colab":{}},"source":["model.fit(X_train_scaled, y_train, epochs=10,\n","          validation_data=(X_valid_scaled, y_valid))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xZuzjyYZp6VB","colab_type":"code","colab":{}},"source":["model.evaluate(X_test_scaled, y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uYQL4I4hp6VG","colab_type":"text"},"source":["### 1.3)\n","Try building and compiling the model again, this time adding `\"mse\"` (or equivalently `\"mean_squared_error\"` or `keras.losses.mean_squared_error`) to the list of additional metrics, then train the model and make sure the `my_mse` is equal to the standard `mse`."]},{"cell_type":"code","metadata":{"id":"HIjXn8yip6VI","colab_type":"code","colab":{}},"source":["model = keras.models.Sequential([\n","    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n","    keras.layers.Dense(1),\n","])\n","model.compile(loss=my_mse, optimizer=keras.optimizers.SGD(lr=1e-3),\n","              metrics=[\"mean_squared_error\"])\n","model.fit(X_train_scaled, y_train, epochs=10,\n","          validation_data=(X_valid_scaled, y_valid))\n","model.evaluate(X_test_scaled, y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iCC9tLrrp6VN","colab_type":"text"},"source":["### 1.4)\n","If you want your code to be portable to other Python implementations of the Keras API, you should use the operations in `keras.backend` rather than TensorFlow operations directly. This package contains thin wrappers around the backend's operations (for example, `keras.backend.square()` simply calls `tf.square()`). Try reimplementing the `my_mse()` function this way and use it to train and evaluate your model again. **Tip**: people frequently define `K = keras.backend` to make their code more readable."]},{"cell_type":"code","metadata":{"id":"xV8OG37pp6VN","colab_type":"code","colab":{}},"source":["def my_portable_mse(y_true, y_pred):\n","    K = keras.backend\n","    return K.mean(K.square(y_pred - y_true))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"oxZ9p1HLp6VQ","colab_type":"code","colab":{}},"source":["model = keras.models.Sequential([\n","    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n","    keras.layers.Dense(1),\n","])\n","model.compile(loss=my_portable_mse,\n","              optimizer=keras.optimizers.SGD(lr=1e-3),\n","              metrics=[\"mean_squared_error\"])\n","model.fit(X_train_scaled, y_train, epochs=10,\n","          validation_data=(X_valid_scaled, y_valid))\n","model.evaluate(X_test_scaled, y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yirclfLlp6VT","colab_type":"text"},"source":["![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"]},{"cell_type":"markdown","metadata":{"id":"r66J772ap6VV","colab_type":"text"},"source":["## Exercise 2 – Custom layer"]},{"cell_type":"markdown","metadata":{"id":"iCjDWmLYp6VW","colab_type":"text"},"source":["### 2.1)\n","Some layers have no weights, such as `keras.layers.Flatten` or `keras.layers.ReLU`. If you want to create a custom layer without any weights, the simplest option is to create a `keras.layers.Lambda` layer and pass it the function to perform. For example, try creating a custom layer that applies the softplus function (log(exp(X) + 1), and try calling this layer like a regular function.\n","\n","**Tip**: you can use `tf.math.softplus()` rather than computing the log and the exponential manually."]},{"cell_type":"code","metadata":{"id":"1HJrjwd9p6VY","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V0se_WqTp6Vc","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4K5kdKFop6Ve","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ddfpVMtYp6Vh","colab_type":"text"},"source":["### 2.2)\n","Create a regression model like in exercise 1, but add your softplus layer at the top (i.e., after the existing 1-unit dense layer). This can be useful to ensure that your model never predicts negative values."]},{"cell_type":"code","metadata":{"id":"WHgDviqSp6Vi","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lS_XzSxvp6Vn","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lirK6Gb4p6Vt","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4QxObmtxp6V5","colab_type":"text"},"source":["### 2.3)\n","Alternatively, try using this softplus layer as the activation function of the output layer.\n","\n","**Notes**:\n","* setting a layer's activation function is just a handy way of adding an extra weightless layer.\n","* Keras supports the softplus activation function out of the box:\n","  * set `activation=\"softplus\"`\n","  * or set `activation=keras.activations.softplus`\n","  * or add a `keras.layers.Activation(\"softplus\")` layer to your model."]},{"cell_type":"code","metadata":{"id":"P6zebBsSp6V8","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tDvKuGp9p6WB","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V7i5rYOEp6WC","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vOGYPHcjp6WH","colab_type":"text"},"source":["### 2.4)\n","Now let's create a custom layer with its own weights. Use the following template to create a `MyDense` layer that computes $\\phi(\\mathbf{X} \\mathbf{W}) + \\mathbf{b}$, where $\\phi$ is the (optional) activation function, $\\mathbf{X}$ is the input data, $\\mathbf{W}$ represents the kernel (i.e., connection weights), and $\\mathbf{b}$ represents the biases, then train and evaluate a model using this instead of a regular `Dense` layer.\n","\n","**Tips**:\n","* The constructor `__init__()`:\n","  * It must have all your layer's hyperparameters as arguments, and save them to instance variables. You will need the number of `units` and the optional `activation` function. To support all kinds of activation functions (strings or functions), simply create a `keras.layers.Activation` passing it the `activation` argument.\n","  * The `**kwargs` argument must be passed to the base class's constructor (`super().__init__()`) so your class can support the `input_shape` argument, and more.\n","* The `build()` method:\n","  * The `build()` method will be called automatically by Keras when it knows the shape of the inputs. Note that the argument should really be called `batch_input_shape` since it includes the batch size.\n","  * You must call `self.add_weight()` for each weight you want to create, specifying its `name`, `shape` (which often depends on the `input_shape`), how to initialize it, and whether or not it is `trainable`. You need two weights: the `kernel` (connection weights) and the `biases`. The kernel must be initialized randomly. The biases are usually initialized with zeros. **Note**: you can find many initializers in `keras.initializers`.\n","  * Do not forget to call `super().build()`, so Keras knows that the model has been built.\n","  * Note: you could create the weights in the constructor, but it is preferable to create them in the `build()` method, because users of your class may not always know the `input_shape` when creating the model. The first time the model is used on some actual data, the `build()` method will automatically be called with the actual `input_shape`.\n","* The `call()` method:\n","  * This is where to code your layer's actual computations. As before, you can use TensorFlow operations directly, or use `keras.backend` operations if you want the layer to be portable to other Keras implementations.\n","* The `compute_output_shape()` method:\n","  * You do not need to implement this method when using tf.keras, as the `Layer` class provides a good implementation.\n","  * However, if want to port your code to another Keras implementation (such as keras-team), and if the output shape is different from the input shape, then you need to implement this method. Note that the input shape is actually the batch input shape, and the ouptut shape must be the batch output shape."]},{"cell_type":"code","metadata":{"id":"pcIhmA_mp6WJ","colab_type":"code","colab":{}},"source":["# This template was copied from https://keras.io/layers/writing-your-own-keras-layers/\n","# I just removed the imports and replaced Layer with keras.layers.Layer.\n","\n","class MyLayer(keras.layers.Layer):\n","    def __init__(self, output_dim, **kwargs):\n","        self.output_dim = output_dim\n","        super(MyLayer, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        # Create a trainable weight variable for this layer.\n","        self.kernel = self.add_weight(name='kernel', \n","                                      shape=(input_shape[1], self.output_dim),\n","                                      initializer='uniform',\n","                                      trainable=True)\n","        super(MyLayer, self).build(input_shape)  # Be sure to call this at the end\n","\n","    def call(self, x):\n","        return K.dot(x, self.kernel)\n","\n","    def compute_output_shape(self, input_shape):\n","        return (input_shape[0], self.output_dim)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6lcoXSK-p6WM","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XrJf6Aoap6WN","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NSwi_bS2p6WP","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VM7RLo7Xp6WQ","colab_type":"text"},"source":["![Exercise solution](https://camo.githubusercontent.com/250388fde3fac9135ead9471733ee28e049f7a37/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f302f30362f46696c6f735f736567756e646f5f6c6f676f5f253238666c69707065642532392e6a7067)"]},{"cell_type":"markdown","metadata":{"id":"lvgOWevVp6WR","colab_type":"text"},"source":["## Exercise 2 – Solution"]},{"cell_type":"markdown","metadata":{"id":"iPtV_FiRp6WR","colab_type":"text"},"source":["### 2.1)\n","Some layers have no weights, such as `keras.layers.Flatten` or `keras.layers.ReLU`. If you want to create a custom layer without any weights, the simplest option is to create a `keras.layers.Lambda` layer and pass it the function to perform. For example, try creating a custom layer that applies the softplus function (log(exp(X) + 1), and try calling this layer like a regular function."]},{"cell_type":"code","metadata":{"id":"M8BbU_rqp6WS","colab_type":"code","colab":{}},"source":["my_softplus = keras.layers.Lambda(lambda X: tf.nn.softplus(X))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sKLgXFkbp6WW","colab_type":"code","colab":{}},"source":["my_softplus([-10., -5., 0., 5., 10.])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LhzV53gpp6Wb","colab_type":"text"},"source":["### 2.2)\n","Create a regression model like in exercise 1, but add your softplus layer at the top (i.e., after the existing 1-unit dense layer). This can be useful to ensure that your model never predicts negative values."]},{"cell_type":"code","metadata":{"id":"eYDMQs_wp6Wc","colab_type":"code","colab":{}},"source":["model = keras.models.Sequential([\n","    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n","    keras.layers.Dense(1),\n","    my_softplus\n","])\n","model.compile(loss=my_portable_mse, optimizer=keras.optimizers.SGD(lr=1e-3))\n","model.fit(X_train_scaled, y_train, epochs=10,\n","          validation_data=(X_valid_scaled, y_valid))\n","model.evaluate(X_test_scaled, y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mAe2Nuflp6We","colab_type":"text"},"source":["### 2.3)\n","Alternatively, try using this softplus layer as the activation function of the output layer."]},{"cell_type":"code","metadata":{"id":"i5gTQ16Yp6Wf","colab_type":"code","colab":{}},"source":["model = keras.models.Sequential([\n","    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n","    keras.layers.Dense(1, activation=my_softplus)\n","#   A few alternatives...\n","#   keras.layers.Dense(1, activation=tf.function(lambda X: my_softplus(X)))\n","#   keras.layers.Dense(1, activation=\"softplus\")\n","#   keras.layers.Dense(1, activation=keras.activations.softplus)\n","#   keras.layers.Dense(1), keras.layers.Activation(\"softplus\")\n","])\n","\n","model.compile(loss=my_portable_mse, optimizer=keras.optimizers.SGD(lr=1e-3))\n","model.fit(X_train_scaled, y_train, epochs=10,\n","          validation_data=(X_valid_scaled, y_valid))\n","model.evaluate(X_test_scaled, y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lmLLoSW0p6Wk","colab_type":"text"},"source":["### 2.4)\n","Now let's create a custom layer with its own weights. Use the following template to create a `MyDense` layer that computes $\\phi(\\mathbf{X} \\mathbf{W}) + \\mathbf{b}$, where $\\phi$ is the (optional) activation function, $\\mathbf{X}$ is the input data, $\\mathbf{W}$ represents the kernel (i.e., connection weights), and $\\mathbf{b}$ represents the biases, then train and evaluate a model using this instead of a regular `Dense` layer."]},{"cell_type":"code","metadata":{"id":"DnfpsNKZp6Wk","colab_type":"code","colab":{}},"source":["class MyDense(keras.layers.Layer):\n","    def __init__(self, units, activation=None, **kwargs):\n","        self.units = units\n","        self.activation = keras.layers.Activation(activation)\n","        super(MyDense, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        self.kernel = self.add_weight(name='kernel', \n","                                      shape=(input_shape[1], self.units),\n","                                      initializer='uniform',\n","                                      trainable=True)\n","        self.biases = self.add_weight(name='bias', \n","                                      shape=(self.units,),\n","                                      initializer='zeros',\n","                                      trainable=True)\n","        super(MyDense, self).build(input_shape)\n","\n","    def call(self, X):\n","        return self.activation(X @ self.kernel + self.biases)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FyFHW-zRp6Wn","colab_type":"code","colab":{}},"source":["model = keras.models.Sequential([\n","    MyDense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n","    MyDense(1)\n","])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"e4WDqJcnp6Wq","colab_type":"code","colab":{}},"source":["model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n","model.fit(X_train_scaled, y_train, epochs=10,\n","          validation_data=(X_valid_scaled, y_valid))\n","model.evaluate(X_test_scaled, y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"77TpRyjzp6Wv","colab_type":"text"},"source":["![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"]},{"cell_type":"markdown","metadata":{"id":"VFz_SAbjp6Ww","colab_type":"text"},"source":["## Exercise 3 – TensorFlow Functions"]},{"cell_type":"markdown","metadata":{"id":"V5k2qMB-p6Wx","colab_type":"text"},"source":["### 3.1)\n","Examine and run the following code examples."]},{"cell_type":"code","metadata":{"id":"BzC81pXXp6Wx","colab_type":"code","colab":{}},"source":["def scaled_elu(z, scale=1.0, alpha=1.0):\n","    is_positive = tf.greater_equal(z, 0.0)\n","    return scale * tf.where(is_positive, z, alpha * tf.nn.elu(z))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Ok5jqrwp6W0","colab_type":"code","colab":{}},"source":["scaled_elu(tf.constant(-3.))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"maydKpGap6W6","colab_type":"code","colab":{}},"source":["scaled_elu(tf.constant([-3., 2.5]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DEFDrX9Zp6W8","colab_type":"code","colab":{}},"source":["scaled_elu_tf = tf.function(scaled_elu)\n","scaled_elu_tf"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o0BUsdz6p6W-","colab_type":"code","colab":{}},"source":["scaled_elu_tf(tf.constant(-3.))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N4OEEdesp6XB","colab_type":"code","colab":{}},"source":["scaled_elu_tf(tf.constant([-3., 2.5]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_43Z6YD5p6XD","colab_type":"code","colab":{}},"source":["scaled_elu_tf.python_function is scaled_elu"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tvqs_zUop6XF","colab_type":"code","colab":{}},"source":["%timeit scaled_elu(tf.random.normal((1000, 1000)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GDnwcq3Ep6XI","colab_type":"code","colab":{}},"source":["%timeit scaled_elu_tf(tf.random.normal((1000, 1000)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QvVrnrUnp6XL","colab_type":"code","colab":{}},"source":["def display_tf_code(func):\n","    from IPython.display import display, Markdown\n","    if hasattr(func, \"python_function\"):\n","        func = func.python_function\n","    code = tf.autograph.to_code(func, experimental_optional_features=None)\n","    display(Markdown('```python\\n{}\\n```'.format(code)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"n_4bKqt2p6XM","colab_type":"code","colab":{}},"source":["display_tf_code(scaled_elu)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NVEmFfZqp6XT","colab_type":"code","colab":{}},"source":["var = tf.Variable(0)\n","\n","@tf.function\n","def add_21():\n","    return var.assign_add(21)\n","\n","@tf.function\n","def times_2():\n","    return var.assign(var * 2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kguyjFiZp6XZ","colab_type":"code","colab":{}},"source":["add_21()\n","times_2()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"24NPY8dCp6Xf","colab_type":"code","colab":{}},"source":["def times_4(x):\n","    return 4. * x\n","\n","@tf.function\n","def times_4_plus_22(x):\n","    return times_4(x) + 22."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uPr5aGDkp6Xi","colab_type":"code","colab":{}},"source":["times_4_plus_22(tf.constant(5.))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tTxwx56tp6Xj","colab_type":"text"},"source":["Compute 1 + 1/2 + 1/4 + ...: the order of execution of the operations with side-effects (e.g., `assign()`) is preserved (in TF 1.x, `tf.control_dependencies()` was needed in such cases):"]},{"cell_type":"code","metadata":{"id":"ZbIdgwYOp6Xk","colab_type":"code","colab":{}},"source":["total = tf.Variable(0.)\n","increment = tf.Variable(1.)\n","\n","@tf.function\n","def converge_to_2(n_iterations):\n","    for i in tf.range(n_iterations):\n","        total.assign_add(increment)\n","        increment.assign(increment / 2.0)\n","    return total\n","\n","converge_to_2(20)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WDhxWOsVp6Xm","colab_type":"text"},"source":["### 3.2)\n","Write a function that computes the sum of squares from 1 to n, where n is an argument. Convert it to a graph function by using `tf.function` as a decorator. Display the code generated by autograph using the `display_tf_code()` function. Use `%timeit` to see how must faster the TensorFlow `Function` is compared to the Python function."]},{"cell_type":"code","metadata":{"id":"_b3E53sYp6Xm","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JaZ_4NC6p6Xo","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hQ17ucs3p6Xr","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WCuYVXX-p6Xu","colab_type":"text"},"source":["### 3.3)\n","Examine and run the following code examples."]},{"cell_type":"code","metadata":{"id":"pGi4kuzsp6Xv","colab_type":"code","colab":{}},"source":["@tf.function\n","def square(x):\n","    tf.print(\"Calling\", x)  # part of the TF Function\n","    print(\"Tracing\")  # NOT part of the TF Function\n","    return tf.square(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LfyhnbTPp6Xz","colab_type":"code","colab":{}},"source":["for i in range(5):\n","    square(tf.constant(i))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4QdmJ49kp6X2","colab_type":"code","colab":{}},"source":["for i in range(5):\n","    square(tf.constant(i, dtype=tf.float32))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IWEZSVV1p6X6","colab_type":"code","colab":{}},"source":["for i in range(5):\n","    square(tf.constant([i, i], dtype=tf.float32))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YiNX4u6bp6YA","colab_type":"code","colab":{}},"source":["# WARNING: when passing non-tensor values, a trace happens for any new value!\n","# This is to allow optimization in case this value determines e.g., number of layers.\n","for i in range(5):\n","    square(i)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wbkvi9NYp6YD","colab_type":"text"},"source":["### 3.4)\n","When you give Keras a custom loss function, it actually creates a graph function based on it, and then uses that graph function during training. The same is true of custom metric functions, and the `call()` method of custom layers and models. Create a `my_mse()` function, like you did earlier, but add an instruction to log a message inside it (use `print()`, *not* `tf.print()`!), and verify that the message is only logged once when you compile and train the model. Optionally, you can also find out when Keras converts custom metrics, layers and models."]},{"cell_type":"code","metadata":{"id":"Gw8J97kCp6YE","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bm9ChufAp6YI","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oHa2oCIJp6YK","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J8bp0hHJp6YM","colab_type":"text"},"source":["### 3.5)\n","Examine the following function, and try to call it with various argument types and shapes. Notice that only tensors of type `int32` and one dimension (of any size) are accepted now that we have specified the `input_signature`."]},{"cell_type":"code","metadata":{"id":"i1ERW3osp6YM","colab_type":"code","colab":{}},"source":["@tf.function(input_signature=[tf.TensorSpec([None], tf.int32, name=\"x\")])\n","def cube(z):\n","    return tf.pow(z, 3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vopl3ZtOp6YO","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cdxfXivcp6YP","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kQ3F6FGNp6YR","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9ztLFEiUp6YS","colab_type":"text"},"source":["![Exercise solution](https://camo.githubusercontent.com/250388fde3fac9135ead9471733ee28e049f7a37/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f302f30362f46696c6f735f736567756e646f5f6c6f676f5f253238666c69707065642532392e6a7067)"]},{"cell_type":"markdown","metadata":{"id":"YJHsMSiop6YS","colab_type":"text"},"source":["## Exercise 3 – Solution"]},{"cell_type":"markdown","metadata":{"id":"M9o2F_zIp6YT","colab_type":"text"},"source":["### 3.1)\n","Examine the code examples."]},{"cell_type":"markdown","metadata":{"id":"tImXLBadp6YT","colab_type":"text"},"source":["Done."]},{"cell_type":"markdown","metadata":{"id":"D2aNFiJMp6YU","colab_type":"text"},"source":["### 3.2)\n","Write a function that computes the sum of squares from 1 to n, where n is an argument. Convert it to a graph function by using `tf.function` as a decorator. Display the code generated by autograph using the `display_tf_code()` function. Use `%timeit` to see how must faster the TensorFlow `Function` is compared to the Python function."]},{"cell_type":"code","metadata":{"id":"bfi9NrxLp6YU","colab_type":"code","colab":{}},"source":["@tf.function\n","def sum_squares(n):\n","    s = tf.constant(0)\n","    for i in range(1, n + 1):\n","        s = s + i ** 2\n","    return s"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SD9BYFLup6YZ","colab_type":"code","colab":{}},"source":["sum_squares(tf.constant(5))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IkHIDhYjp6Yb","colab_type":"code","colab":{}},"source":["display_tf_code(sum_squares.python_function)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RO3K-qs8p6Yh","colab_type":"code","colab":{}},"source":["%timeit sum_squares(10000)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pVVi0n1Ep6Yk","colab_type":"code","colab":{}},"source":["%timeit sum_squares.python_function(10000)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tRP234A6p6Ym","colab_type":"text"},"source":["### 3.3)\n","Examine the code examples."]},{"cell_type":"markdown","metadata":{"id":"ZphDgQykp6Yn","colab_type":"text"},"source":["Done."]},{"cell_type":"markdown","metadata":{"id":"7q_HQHtSp6Yn","colab_type":"text"},"source":["### 3.4)\n","When you give Keras a custom loss function, it actually creates a graph function based on it, and then uses that graph function during training. The same is true of custom metric functions, and the `call()` method of custom layers and models. Create a `my_mse()` function, like you did earlier, but add an instruction to log a message inside it (use `print()`, *not* `tf.print()`!), and verify that the message is only logged once when you compile and train the model. Optionally, you can also find out when Keras converts custom metrics, layers and models."]},{"cell_type":"code","metadata":{"id":"YXip2-6Pp6Yo","colab_type":"code","colab":{}},"source":["# Custom loss function\n","def my_mse(y_true, y_pred):\n","    print(\"Tracing loss my_mse()\")\n","    return tf.reduce_mean(tf.square(y_pred - y_true))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1OVPXlAmp6Yq","colab_type":"code","colab":{}},"source":["# Custom metric function\n","def my_mae(y_true, y_pred):\n","    print(\"Tracing metric my_mae()\")\n","    return tf.reduce_mean(tf.abs(y_pred - y_true))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3CC2-sLgp6Ys","colab_type":"code","colab":{}},"source":["# Custom layer\n","class MyDense(keras.layers.Layer):\n","    def __init__(self, units, activation=None, **kwargs):\n","        self.units = units\n","        self.activation = keras.layers.Activation(activation)\n","        super(MyDense, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        self.kernel = self.add_weight(name='kernel', \n","                                      shape=(input_shape[1], self.units),\n","                                      initializer='uniform',\n","                                      trainable=True)\n","        self.biases = self.add_weight(name='bias', \n","                                      shape=(self.units,),\n","                                      initializer='zeros',\n","                                      trainable=True)\n","        super(MyDense, self).build(input_shape)\n","\n","    def call(self, X):\n","        print(\"Tracing MyDense.call()\")\n","        return self.activation(X @ self.kernel + self.biases)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BGie0MKxp6Yx","colab_type":"code","colab":{}},"source":["# Custom model\n","class MyModel(keras.models.Model):\n","    def __init__(self):\n","        super(MyModel, self).__init__()\n","        self.hidden1 = MyDense(30, activation=\"relu\")\n","        self.hidden2 = MyDense(30, activation=\"relu\")\n","        self.output_ = MyDense(1)\n","\n","    def call(self, input):\n","        print(\"Tracing MyModel.call()\")\n","        hidden1 = self.hidden1(input)\n","        hidden2 = self.hidden2(hidden1)\n","        concat = keras.layers.concatenate([input, hidden2])\n","        output = self.output_(concat)\n","        return output\n","\n","model = MyModel()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-PNyBDI7p6Yz","colab_type":"code","colab":{}},"source":["model.compile(loss=my_mse, optimizer=keras.optimizers.SGD(lr=1e-3),\n","              metrics=[my_mae])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EACsWUAPp6Y-","colab_type":"code","colab":{}},"source":["model.fit(X_train_scaled, y_train, epochs=2,\n","          validation_data=(X_valid_scaled, y_valid))\n","model.evaluate(X_test_scaled, y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kI7ujHf5p6ZA","colab_type":"text"},"source":["Notice that each custom function is traced just once, except for the metric function. That's a bit odd."]},{"cell_type":"markdown","metadata":{"id":"auDCmOhcp6ZB","colab_type":"text"},"source":["### 3.5)\n","Examine the following function, and try to call it with various argument types and shapes. Notice that only tensors of type `int32` and one dimension (of any size) are accepted now that we have specified the `input_signature`."]},{"cell_type":"code","metadata":{"id":"D3RNWu_Vp6ZE","colab_type":"code","colab":{}},"source":["@tf.function(input_signature=[tf.TensorSpec([None], tf.int32, name=\"x\")])\n","def cube(z):\n","    return tf.pow(z, 3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SeJeKq3Zp6ZF","colab_type":"code","colab":{}},"source":["cube(tf.constant([1, 2, 3]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lhGe1AXvp6ZI","colab_type":"code","colab":{}},"source":["cube(tf.constant([1, 2, 3, 4, 5]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6t_vnALKp6ZO","colab_type":"code","colab":{}},"source":["try:\n","    cube([1, 2, 3])\n","except ValueError as ex:\n","    print(ex)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e9XLoEDnp6ZR","colab_type":"code","colab":{}},"source":["try:\n","    cube(tf.constant([1., 2., 3]))\n","except ValueError as ex:\n","    print(ex)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"5WIgNZQcp6ZS","colab_type":"code","colab":{}},"source":["try:\n","    cube(tf.constant([[1, 2], [3, 4]]))\n","except ValueError as ex:\n","    print(ex)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GP2CIooUp6ZT","colab_type":"text"},"source":["![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"]},{"cell_type":"markdown","metadata":{"id":"S7XeXGMep6ZU","colab_type":"text"},"source":["## Exercise 4 – Function Graphs"]},{"cell_type":"markdown","metadata":{"id":"im0euHQup6ZV","colab_type":"text"},"source":["### 4.1)\n","Examine and run the following code examples."]},{"cell_type":"code","metadata":{"id":"FqS1cScYp6ZY","colab_type":"code","colab":{}},"source":["@tf.function(input_signature=[tf.TensorSpec([None], tf.int32, name=\"x\")])\n","def cube(z):\n","    return tf.pow(z, 3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kEz5zBoSp6ZZ","colab_type":"code","colab":{}},"source":["cube_func_int32 = cube.get_concrete_function(tf.TensorSpec([None], tf.int32))\n","cube_func_int32"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v3GmXBp6p6Zb","colab_type":"code","colab":{}},"source":["cube_func_int32 is cube.get_concrete_function(tf.TensorSpec([5], tf.int32))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vkt_6KpYp6Zc","colab_type":"code","colab":{}},"source":["cube_func_int32 is cube.get_concrete_function(tf.constant([1, 2, 3]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"O3uNHMlbp6Zd","colab_type":"code","colab":{}},"source":["cube_func_int32.graph"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FmvqxxsJp6Ze","colab_type":"text"},"source":["### 4.2)\n","The function's graph is represented on the following diagram. Call the graph's `get_operations()` method to get the list of operations. Each operation has an `inputs` attribute that returns an iterator over its input tensors (these are symbolic: contrary to tensors we have used up to now, they have no value). It also has an `outputs` attribute that returns the list of output tensors. Each tensor has an `op` attribute that returns the operation it comes from. Try navigating through the graph using these methods and attributes."]},{"cell_type":"markdown","metadata":{"id":"vUkfcnnyp6Zf","colab_type":"text"},"source":["<img src=\"https://github.com/10sorflow/tf2_course/blob/master/images/cube_graph.png?raw=1\" width=\"600\" />"]},{"cell_type":"code","metadata":{"id":"OWX0x9rEp6Zg","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6tp1Z1Fbp6Zj","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bRXrbAx9p6Zk","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c5dt4jHfp6Zm","colab_type":"text"},"source":["### 4.3)\n","Each operation has a default name, such as `\"pow\"` (you can override it by setting the `name` attribute when you call the operation). In case of a name conflict, TensorFlow adds an underscore and anindex to make the name unique (e.g. `\"pow_1\"`). Moreover, each tensor has the same name as the operation that outputs it, followed by a colon `:` and the tensor's `index` (e.g., `\"pow:0\"`). Most operations have a single output tensor, so most tensors have a name that ends with `:0`. Try using `get_operation_by_name()` and `get_tensor_by_name()` to access any op and tensor you wish."]},{"cell_type":"code","metadata":{"id":"bjicTBRTp6Zm","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"s1Gz7kQFp6Zo","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QfCQlaAap6Zp","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n_B_DEkip6Zq","colab_type":"text"},"source":["### 4.4)\n","Call the graph's `as_graph_def()` method and print the output. This is a protobuf representation of the computation graph: it is what makes TensorFlow models so portable."]},{"cell_type":"code","metadata":{"id":"xsuqXmXup6Zr","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"k_WYnBArp6Zt","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5LdBMVA3p6Zw","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W4-pj8V2p6Zx","colab_type":"text"},"source":["### 4.5)\n","Get the concrete function's `function_def`, and look at its `signature`. This shows the names and types of the nodes in the graph that correspond to the function's inputs and outputs. This will come in handy when you deploy models to TensorFlow Serving or Google Cloud ML Engine."]},{"cell_type":"code","metadata":{"id":"RN2we8Kmp6Zy","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"z96BkSRcp6Z2","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ub_X9RxIp6Z3","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hKMuih6qp6Z4","colab_type":"text"},"source":["![Exercise solution](https://camo.githubusercontent.com/250388fde3fac9135ead9471733ee28e049f7a37/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f302f30362f46696c6f735f736567756e646f5f6c6f676f5f253238666c69707065642532392e6a7067)"]},{"cell_type":"markdown","metadata":{"id":"JPrJEbcip6Z4","colab_type":"text"},"source":["## Exercise 4 – Solution"]},{"cell_type":"markdown","metadata":{"id":"oD-q90LRp6Z9","colab_type":"text"},"source":["### 4.1)\n","Examine the code examples."]},{"cell_type":"markdown","metadata":{"id":"JeQXKW6wp6Z-","colab_type":"text"},"source":["Done."]},{"cell_type":"markdown","metadata":{"id":"QcPIfVWFp6Z-","colab_type":"text"},"source":["### 4.2)\n","The function's graph is represented on the following diagram. Call the graph's `get_operations()` method to get the list of operations. Each operation has an `inputs` attribute that returns an iterator over its input tensors (these are symbolic: contrary to tensors we have used up to now, they have no value). It also has an `outputs` attribute that returns the list of output tensors. Each tensor has an `op` attribute that returns the operation it comes from. Try navigating through the graph using these methods and attributes."]},{"cell_type":"markdown","metadata":{"id":"Ky8h_Ir7p6Z_","colab_type":"text"},"source":["<img src=\"https://github.com/10sorflow/tf2_course/blob/master/images/cube_graph.png?raw=1\" width=\"600\" />"]},{"cell_type":"code","metadata":{"id":"LW5YIE9Cp6aB","colab_type":"code","colab":{}},"source":["cube_func_int32.graph.get_operations()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jCiJ7QY4p6aC","colab_type":"code","colab":{}},"source":["pow_op = cube_func_int32.graph.get_operations()[2]\n","pow_op"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pwGa_z-Ip6aE","colab_type":"code","colab":{}},"source":["pow_in = list(pow_op.inputs)\n","pow_in"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AMqrcoBhp6aF","colab_type":"code","colab":{}},"source":["pow_out = list(pow_op.outputs)\n","pow_out"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nPU0WrU_p6aG","colab_type":"code","colab":{}},"source":["pow_in = list(pow_op.inputs)\n","pow_in"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"I_Wzj3Kzp6aH","colab_type":"code","colab":{}},"source":["pow_in[0].op"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rQwSZ4nMp6aI","colab_type":"text"},"source":["### 4.3)\n","Each operation has a default name, such as `\"pow\"` (you can override it by setting the `name` attribute when you call the operation). In case of a name conflict, TensorFlow adds an underscore and anindex to make the name unique (e.g. `\"pow_1\"`). Moreover, each tensor has the same name as the operation that outputs it, followed by a colon `:` and the tensor's `index` (e.g., `\"pow:0\"`). Most operations have a single output tensor, so most tensors have a name that ends with `:0`. Try using `get_operation_by_name()` and `get_tensor_by_name()` to access any op and tensor you wish."]},{"cell_type":"code","metadata":{"id":"qbfuxghop6aJ","colab_type":"code","colab":{}},"source":["cube_func_int32.graph.get_operation_by_name(\"x\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"C7AXhZw-p6aK","colab_type":"code","colab":{}},"source":["cube_func_int32.graph.get_tensor_by_name(\"x:0\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dbq_V5v-p6aL","colab_type":"text"},"source":["### 4.4)\n","Call the graph's `as_graph_def()` method and print the output. This is a protobuf representation of the computation graph: it is what makes TensorFlow models so portable."]},{"cell_type":"code","metadata":{"id":"XRFcApOwp6aL","colab_type":"code","colab":{}},"source":["cube_func_int32.graph.as_graph_def()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n4FICqpEp6aM","colab_type":"text"},"source":["### 4.5)\n","Get the concrete function's `function_def`, and look at its `signature`. This shows the names and types of the nodes in the graph that correspond to the function's inputs and outputs. This will come in handy when you deploy models to TensorFlow Serving or Google Cloud ML Engine."]},{"cell_type":"code","metadata":{"scrolled":false,"id":"3c4gGKx9p6aM","colab_type":"code","colab":{}},"source":["cube_func_int32.function_def.signature"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EUwetha3p6aO","colab_type":"text"},"source":["![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"]},{"cell_type":"markdown","metadata":{"id":"0JS6QHybp6aO","colab_type":"text"},"source":["## Exercise 5 – Autodiff"]},{"cell_type":"markdown","metadata":{"id":"Kpz84Pqvp6aO","colab_type":"text"},"source":["### 5.1)\n","Examine and run the following code examples."]},{"cell_type":"code","metadata":{"id":"oUc-X7fpp6aP","colab_type":"code","colab":{}},"source":["def f(x):\n","    return 3. * x ** 2 + 2. * x - 1."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OK_UWfHkp6aQ","colab_type":"code","colab":{}},"source":["def approximate_derivative(f, x, eps=1e-3):\n","    return (f(x + eps) - f(x - eps)) / (2. * eps)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1jNHC45Bp6aR","colab_type":"code","colab":{}},"source":["approximate_derivative(f, 1.0) # true derivative = 8"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sCUO0Brdp6aS","colab_type":"code","colab":{}},"source":["xs = np.linspace(-2, 2, 200)\n","fs = f(xs)\n","x0 = 0.5\n","df_x0 = approximate_derivative(f, x0)\n","tangent_x0 = df_x0 * (xs - x0) + f(x0)\n","plt.plot([-2, 2], [0, 0], \"k-\", linewidth=1)\n","plt.plot([0, 0], [-5, 15], \"k-\", linewidth=1)\n","plt.plot(xs, fs)\n","plt.plot(xs, tangent_x0, \"r--\")\n","plt.plot(x0, f(x0), \"ro\")\n","plt.grid(True)\n","plt.xlabel(\"x\", fontsize=14)\n","plt.ylabel(\"f(x)\", fontsize=14, rotation=0)\n","plt.axis([-2, 2, -5, 15])\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1gvIWQbFp6aU","colab_type":"code","colab":{}},"source":["def g(x1, x2):\n","    return (x1 + 5) * (x2 ** 2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"INhCBGmfp6aV","colab_type":"code","colab":{}},"source":["def approximate_gradient(f, x1, x2, eps=1e-3):\n","    df_x1 = approximate_derivative(lambda x: f(x, x2), x1, eps)\n","    df_x2 = approximate_derivative(lambda x: f(x1, x), x2, eps)\n","    return df_x1, df_x2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M6YG3Ybqp6aZ","colab_type":"code","colab":{}},"source":["approximate_gradient(g, 2.0, 3.0) # true gradient = (9, 42)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"atY6b6-Gp6aa","colab_type":"code","colab":{}},"source":["x1 = tf.Variable(2.0)\n","x2 = tf.Variable(3.0)\n","with tf.GradientTape() as tape:\n","    z = g(x1, x2)\n","grads = tape.gradient(z, [x1, x2])\n","grads"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fVrZ4tkEp6ab","colab_type":"code","colab":{}},"source":["x1 = tf.Variable(2.0)\n","x2 = tf.Variable(3.0)\n","with tf.GradientTape() as tape:\n","    z = g(x1, x2)\n","\n","dz_x1 = tape.gradient(z, x1)\n","try:\n","    dz_x2 = tape.gradient(z, x2)\n","except RuntimeError as ex:\n","    print(ex)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8mEFHT6Hp6ae","colab_type":"code","colab":{}},"source":["x1 = tf.Variable(2.0)\n","x2 = tf.Variable(3.0)\n","with tf.GradientTape(persistent=True) as tape:\n","    z = g(x1, x2)\n","\n","dz_x1 = tape.gradient(z, x1)\n","dz_x2 = tape.gradient(z, x2)\n","del tape\n","dz_x1, dz_x2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YE_k9pB3p6ag","colab_type":"code","colab":{}},"source":["x1 = tf.constant(2.0) # <= not Variable\n","x2 = tf.constant(3.0) # <= not Variable\n","with tf.GradientTape() as tape:\n","    z = g(x1, x2)\n","\n","grads = tape.gradient(z, [x1, x2])\n","grads"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lt0QW1lYp6ai","colab_type":"code","colab":{}},"source":["x1 = tf.constant(2.0)\n","x2 = tf.constant(3.0)\n","with tf.GradientTape() as tape:\n","    tape.watch(x1)\n","    tape.watch(x2)\n","    z = g(x1, x2)\n","\n","grads = tape.gradient(z, [x1, x2])\n","grads"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HnvbABApp6ak","colab_type":"code","colab":{}},"source":["x = tf.Variable(5.0)\n","with tf.GradientTape() as tape:\n","    z1 = 3 * x\n","    z2 = x ** 2\n","tape.gradient([z1, z2], x) # dz1_x + dz2_x = 3 + 2x = 3 + 2*5 = 13"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jzqxT8Kup6an","colab_type":"code","colab":{}},"source":["x1 = tf.Variable(2.0)\n","x2 = tf.Variable(3.0)\n","with tf.GradientTape(persistent=True) as hessian_tape:\n","    with tf.GradientTape() as jacobian_tape:\n","        z = g(x1, x2)\n","    jacobians = jacobian_tape.gradient(z, [x1, x2])\n","hessians = [hessian_tape.gradient(jacobian, [x1, x2])\n","            for jacobian in jacobians]\n","del hessian_tape\n","hessians"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4geASMTXp6ap","colab_type":"text"},"source":["### 5.2)\n","Implement Gradient Descent manually to find the value of `x` that minimizes the following function `f(x)`.\n","\n","**Tips**:\n","* Define a variable `x` and initialize it to 0.\n","* Define the `learning_rate` (e.g., 0.1).\n","* Write a loop that will repeatedly (1) compute the gradient of `f` (actually a derivative in this case) at the current value of `x`, and (2) tweak `x` slightly in the opposite direction (by subtracting `learning_rate * df_dx`). You can use `x.assign_sub(...)` for this.\n","* Using calculus, we can find that the algorithm should converge to $x = -\\frac{1}{3}$. Indeed, the derivative of $f(x) = 3 x^2 + 2x -1$ is $f'(x) = 6x + 2$, so the minimum is reached when $f'(x) = 0$ (slope is 0), so $6x + 2 = 0$, which leads to $x = -\\frac{1}{3}$.\n"]},{"cell_type":"code","metadata":{"id":"e6kXJP4cp6aq","colab_type":"code","colab":{}},"source":["def f(x):\n","    return 3. * x ** 2 + 2. * x - 1."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"En6yu7MDp6as","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jhbQXK9Xp6at","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"w1HY41iAp6au","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iMAc2cIup6av","colab_type":"text"},"source":["### 5.3)\n","Now use an `SGD` optimizer instead of manually tweaking `x`.\n","\n","**Tips**:\n","* You first need to create an `SGD` optimizer, optionally specifying the learning_rate (e.g., `lr=0.1`).\n","* Next replace the manual tweaking of `x` in your previous code to use `optimizer.apply_gradients()` instead. You need to pass it a list of gradient/variable pairs (just one pair in this example)."]},{"cell_type":"code","metadata":{"id":"8M7TgS6Bp6aw","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ms38qPLDp6ax","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YuNKQcLjp6ay","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mPpJ1kWhp6a0","colab_type":"text"},"source":["### 5.4)\n","Create a `Sequential` model for the California housing problem (no need to compile it), and train it using your own training loop, instead of using `fit()`. Evaluate your model on the validation set at the end of each epoch, and display the result.\n","\n","**Tips**:\n","* You can use the following `random_batch()` function to get a new batch of training data at each iteration (the Data API would be much preferable, as we will see in the next notebook).\n","* You can use the model like a function to make predictions: `y_pred = model(X_batch)`\n","* You can use `keras.losses.mean_squared_error()` to compute the loss. Note that it returns one loss per instance, so you need to use `tf.reduce_mean()` to get the mean loss. \n","* You can use `model.trainable_variables` to get the full list of trainable variables in your model.\n","* You can use `zip(gradients, variables)` to create a list containing all the gradient/variable pairs."]},{"cell_type":"code","metadata":{"id":"5KUMJaeNp6a0","colab_type":"code","colab":{}},"source":["def random_batch(X, y, batch_size = 32):\n","    idx = np.random.randint(0, len(X), size=batch_size)\n","    return X[idx], y[idx]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EyEuyVTCp6a1","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KWuuPWEYp6a4","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JGLHSY9hp6a8","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7V5ne-3Lp6a_","colab_type":"text"},"source":["### 5.5)\n","Examine and run the following code examples, then update your training loop to display the training loss at each iteration.\n","\n","**Tips**:\n","* You can use a `keras.metrics.MeanSquaredError` instance to efficiently track the running mean squared error at each iteration.\n","* Make sure you reset the metric's states at the start of each epoch.\n","* You can use `print(\"\\r\", mse, end=\"\")` to display the MSE on the same line at each iteration."]},{"cell_type":"code","metadata":{"id":"m4RwO3cop6a_","colab_type":"code","colab":{}},"source":["metric = keras.metrics.MeanSquaredError()\n","metric([5.], [2.])  # error = (2 - 5)**2 = 9\n","metric([0.], [1.])  # error = (1 - 0)**2 = 1\n","metric.result()     # mean error = (9 + 1) / 2 = 5"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rNGwRPxqp6bC","colab_type":"code","colab":{}},"source":["metric.reset_states()\n","metric.result()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JAIjQ_Kxp6bE","colab_type":"code","colab":{}},"source":["metric([1.], [3.])  # error = (3 - 1)**2 = 4\n","metric.result()     # mean error = 4 / 1 = 4"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JIj0eQjDp6bG","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OC99dec-p6bI","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q8bRubXEp6bK","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ftQJ8vAHp6bM","colab_type":"text"},"source":["![Exercise solution](https://camo.githubusercontent.com/250388fde3fac9135ead9471733ee28e049f7a37/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f302f30362f46696c6f735f736567756e646f5f6c6f676f5f253238666c69707065642532392e6a7067)"]},{"cell_type":"markdown","metadata":{"id":"reDOANH4p6bM","colab_type":"text"},"source":["## Exercise 5 – Solution"]},{"cell_type":"markdown","metadata":{"id":"ZnDb2ceqp6bM","colab_type":"text"},"source":["### 5.1)\n","Examine the code examples."]},{"cell_type":"markdown","metadata":{"id":"D2ryTwN-p6bN","colab_type":"text"},"source":["Done"]},{"cell_type":"markdown","metadata":{"id":"3X3VSVwfp6bN","colab_type":"text"},"source":["### 5.2)\n","Implement Gradient Descent manually to find the value of `x` that minimizes the following function `f(x)`."]},{"cell_type":"code","metadata":{"id":"jmDLToAsp6bN","colab_type":"code","colab":{}},"source":["def f(x):\n","    return 3. * x ** 2 + 2. * x - 1."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vJDXLpPap6bP","colab_type":"code","colab":{}},"source":["learning_rate = 0.1\n","x = tf.Variable(0.0)\n","\n","for iteration in range(100):\n","    with tf.GradientTape() as tape:\n","        z = f(x)\n","    dz_dx = tape.gradient(z, x)\n","    x.assign_sub(learning_rate * dz_dx)\n","x"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OWuOZwn_p6bQ","colab_type":"text"},"source":["### 5.3)\n","Now use an `SGD` optimizer instead of manually tweaking `x`."]},{"cell_type":"code","metadata":{"id":"O8c4zQwwp6bQ","colab_type":"code","colab":{}},"source":["x = tf.Variable(0.0)\n","optimizer = keras.optimizers.SGD(lr=0.1)\n","\n","for iteration in range(100):\n","    with tf.GradientTape() as tape:\n","        z = f(x)\n","    dz_dx = tape.gradient(z, x)\n","    optimizer.apply_gradients([(dz_dx, x)])\n","x"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JgFeWQY5p6bR","colab_type":"text"},"source":["### 5.4)\n","Create a `Sequential` model for the California housing problem (no need to compile it), and train it using your own training loop, instead of using `fit()`. Evaluate your model on the validation set at the end of each epoch, and display the result."]},{"cell_type":"code","metadata":{"id":"-Yh0WeS7p6bR","colab_type":"code","colab":{}},"source":["def random_batch(X, y, batch_size = 32):\n","    idx = np.random.randint(0, len(X), size=batch_size)\n","    return X[idx], y[idx]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bof0lSAZp6bT","colab_type":"code","colab":{}},"source":["epochs = 10\n","batch_size = 32\n","steps_per_epoch = len(X_train) // batch_size\n","optimizer = keras.optimizers.SGD(lr=1e-3)\n","loss_fn = keras.losses.mean_squared_error\n","\n","model = keras.models.Sequential([\n","    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n","    keras.layers.Dense(1)\n","])\n","\n","for epoch in range(epochs):\n","    for step in range(steps_per_epoch):\n","        X_batch, y_batch = random_batch(X_train_scaled, y_train, batch_size)\n","        with tf.GradientTape() as tape:\n","            y_pred = model(X_batch)\n","            loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n","        grads = tape.gradient(loss, model.variables)\n","        grads_and_vars = zip(grads, model.variables)\n","        optimizer.apply_gradients(grads_and_vars)\n","    y_pred = model(X_valid_scaled)\n","    valid_loss = tf.reduce_mean(loss_fn(y_valid, y_pred))\n","    print(\"Epoch\", epoch, \"valid mse:\", valid_loss.numpy())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NMaIocFSp6bU","colab_type":"text"},"source":["### 5.5)\n","Examine and run the following code examples, then update your training loop to display the training loss at each iteration."]},{"cell_type":"code","metadata":{"id":"H-zkdOw7p6bU","colab_type":"code","colab":{}},"source":["metric = keras.metrics.MeanSquaredError()\n","metric([5.], [2.])  # error = (2 - 5)**2 = 9\n","metric([0.], [1.])  # error = (1 - 0)**2 = 1\n","metric.result()     # mean error = (9 + 1) / 2 = 5"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t_yDCDAdp6bV","colab_type":"code","colab":{}},"source":["metric.reset_states()\n","metric.result()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"u57Haxvup6bX","colab_type":"code","colab":{}},"source":["metric([1.], [3.])  # error = (3 - 1)**2 = 4\n","metric.result()     # mean error = 4 / 1 = 4"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"AUtMy6Jdp6bc","colab_type":"code","colab":{}},"source":["epochs = 10\n","batch_size = 32\n","steps_per_epoch = len(X_train) // batch_size\n","optimizer = keras.optimizers.SGD(lr=1e-3)\n","loss_fn = keras.losses.mean_squared_error\n","metric = keras.metrics.MeanSquaredError()  # ADDED\n","\n","model = keras.models.Sequential([\n","    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n","    keras.layers.Dense(1)\n","])\n","\n","for epoch in range(epochs):\n","    metric.reset_states()  # ADDED\n","    for step in range(steps_per_epoch):\n","        X_batch, y_batch = random_batch(X_train_scaled, y_train, batch_size)\n","        with tf.GradientTape() as tape:\n","            y_pred = model(X_batch)\n","            loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n","            metric(y_batch, y_pred)  # ADDED\n","        grads = tape.gradient(loss, model.trainable_variables)\n","        grads_and_vars = zip(grads, model.trainable_variables)\n","        optimizer.apply_gradients(grads_and_vars)\n","        print(\"\\rEpoch\", epoch, \" train mse:\", metric.result().numpy(), end=\"\")  # ADDED\n","    y_pred = model(X_valid_scaled)\n","    valid_loss = tf.reduce_mean(loss_fn(y_valid, y_pred))\n","    print(\"\\tvalid mse:\", valid_loss.numpy())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nqmhe9Dyp6bd","colab_type":"text"},"source":["## Conclusion"]},{"cell_type":"markdown","metadata":{"id":"KUQDjLekp6bd","colab_type":"text"},"source":["Great! You now know how to use TensorFlow's low-level API to write custom loss functions, layers, and models. You also learned how to optimize your functions by converting them to graphs: this allows TensorFlow to run operations in parallel and to perform various optimizations. Next, you learned how TensorFlow Functions and graphs are structured, and how to navigate through them. Finally, you learned how to use autodiff and write your own custom training loops."]}]}